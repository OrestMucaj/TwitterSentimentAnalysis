{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 92791,
          "databundleVersionId": 11083833,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 30918,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "Sentiment_Analysis_LogisticRegression_TF-IDF",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OrestMucaj/TwitterSentimentAnalysis/blob/main/TF-IDF_LogisticRegression/Sentiment_Analysis_LogisticRegression_TF_IDF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "nNonxkZySuTo"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "ai_2_deep_learning_for_nlp_homework_1_path = kagglehub.competition_download('ai-2-deep-learning-for-nlp-homework-1')\n",
        "packagemanager_pm_79148595_at_03_25_2025_20_17_17_path = kagglehub.notebook_output_download('packagemanager/pm-79148595-at-03-25-2025-20-17-17')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "1jkVwcVESuTr"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ΣΤΟΙΧΕΙΑ**\n",
        "ΟΡΕΣΤ ΜΟΥΤΣΑΙ - 1115201900120 - ΠΡΟΠΤΥΧΙΑΚΟΣ"
      ],
      "metadata": {
        "id": "fFLNIf2sSuTs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Imports**"
      ],
      "metadata": {
        "id": "WsMfxlL1SuTt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the Required Libraries And Functions\n",
        "\n",
        "!pip install contractions\n",
        "!pip install emoji\n",
        "!pip install wordcloud\n",
        "!pip install matplotlib\n",
        "!pip install wordcloud\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "import emoji\n",
        "from contractions import fix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.sparse import vstack\n",
        "from wordcloud import WordCloud\n",
        "from collections import Counter\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, roc_auc_score\n",
        "from sklearn.model_selection import learning_curve, PredefinedSplit\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "RANDOMSEED = 21"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-25T20:10:41.669385Z",
          "iopub.execute_input": "2025-03-25T20:10:41.669793Z",
          "iopub.status.idle": "2025-03-25T20:11:07.940519Z",
          "shell.execute_reply.started": "2025-03-25T20:10:41.669743Z",
          "shell.execute_reply": "2025-03-25T20:11:07.939175Z"
        },
        "id": "JFTtuGb4SuTt"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "\n",
        "train_df = pd.read_csv('/kaggle/input/ai-2-deep-learning-for-nlp-homework-1/train_dataset.csv')\n",
        "test_df = pd.read_csv('/kaggle/input/ai-2-deep-learning-for-nlp-homework-1/test_dataset.csv')\n",
        "val_df = pd.read_csv('/kaggle/input/ai-2-deep-learning-for-nlp-homework-1/val_dataset.csv')"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-25T20:11:07.941831Z",
          "iopub.execute_input": "2025-03-25T20:11:07.942277Z",
          "iopub.status.idle": "2025-03-25T20:11:08.564158Z",
          "shell.execute_reply.started": "2025-03-25T20:11:07.942247Z",
          "shell.execute_reply": "2025-03-25T20:11:08.563103Z"
        },
        "id": "L_8O43ICSuTu"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DATA ANALYSIS**"
      ],
      "metadata": {
        "id": "-kQ_mHPqSuTu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get The Most Common Words And The Number Of Unique And Total Words Before Preprocessing\n",
        "\n",
        "print(\"ANALYSING DATA BEFORE PRE-PROCESSING THE DATASET\\n\")\n",
        "\n",
        "def get_most_common_words(text_series, top_n=10, exclude_stopwords=True):\n",
        "    all_words = []\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    for text in text_series.dropna():\n",
        "        tokens = word_tokenize(text)  # Removed lowercasing\n",
        "\n",
        "        if exclude_stopwords:\n",
        "            words = [word for word in tokens if word.lower() not in stop_words]\n",
        "        else:\n",
        "            words = tokens\n",
        "\n",
        "        all_words.extend(words)\n",
        "\n",
        "    word_counts = Counter(all_words)\n",
        "    return word_counts.most_common(top_n)\n",
        "\n",
        "def count_words(text_series, exclude_stopwords=True, unique=True):\n",
        "    all_words = []\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    for text in text_series.dropna():\n",
        "        tokens = word_tokenize(text)\n",
        "\n",
        "        if exclude_stopwords:\n",
        "            words = [word for word in tokens if word.lower() not in stop_words]\n",
        "        else:\n",
        "            words = tokens\n",
        "\n",
        "        all_words.extend(words)\n",
        "\n",
        "    return len(set(all_words)) if unique else len(all_words)\n",
        "\n",
        "common_words = get_most_common_words(train_df['Text'], top_n=10, exclude_stopwords=False)\n",
        "df = pd.DataFrame(common_words, columns=[\"Word\", \"Frequency\"])\n",
        "\n",
        "print(\"For the training dataset:\\n\", df.to_string(index=False), \"\\n\")\n",
        "\n",
        "common_words = get_most_common_words(val_df['Text'], top_n=10, exclude_stopwords=False)\n",
        "df = pd.DataFrame(common_words, columns=[\"Word\", \"Frequency\"])\n",
        "print(\"For the validation dataset:\\n\", df.to_string(index=False), \"\\n\")\n",
        "\n",
        "common_words = get_most_common_words(test_df['Text'], top_n=10, exclude_stopwords=False)\n",
        "df = pd.DataFrame(common_words, columns=[\"Word\", \"Frequency\"])\n",
        "print(\"For the testing dataset:\\n\", df.to_string(index=False), \"\\n\")\n",
        "\n",
        "vocab_size = count_words(train_df['Text'], exclude_stopwords=False, unique=True)\n",
        "print(f\"Unique word count in the training dataset: {vocab_size}\")\n",
        "\n",
        "vocab_size = count_words(train_df['Text'], exclude_stopwords=False, unique=False)\n",
        "print(f\"Total word count in the training dataset: {vocab_size}\\n\")\n",
        "\n",
        "vocab_size = count_words(val_df['Text'],exclude_stopwords=False, unique=True)\n",
        "print(f\"Unique word count in the validation dataset: {vocab_size}\")\n",
        "\n",
        "vocab_size = count_words(val_df['Text'],exclude_stopwords=False, unique=False)\n",
        "print(f\"Total word count in the validation dataset: {vocab_size}\\n\")\n",
        "\n",
        "vocab_size = count_words(test_df['Text'], exclude_stopwords=False, unique=True)\n",
        "print(f\"Unique word count in the testing dataset: {vocab_size}\")\n",
        "\n",
        "vocab_size = count_words(test_df['Text'],exclude_stopwords=False, unique=False)\n",
        "print(f\"Total word count in the testing dataset: {vocab_size}\\n\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-25T20:11:08.565152Z",
          "iopub.execute_input": "2025-03-25T20:11:08.565487Z",
          "iopub.status.idle": "2025-03-25T20:13:08.42161Z",
          "shell.execute_reply.started": "2025-03-25T20:11:08.56546Z",
          "shell.execute_reply": "2025-03-25T20:13:08.420572Z"
        },
        "id": "j2HosJC-SuTv"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate Word Cloud Before Preprocessing\n",
        "\n",
        "text = \" \".join(str(word) for word in train_df['Text'])\n",
        "\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.savefig('/kaggle/working/train_word_cloud_before.png')\n",
        "plt.show()\n",
        "\n",
        "text = \" \".join(str(word) for word in val_df['Text'])\n",
        "\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.savefig('/kaggle/working/val_word_cloud_before.png')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "text = \" \".join(str(word) for word in test_df['Text'])\n",
        "\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.savefig('/kaggle/working/test_word_cloud_before.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-25T20:13:08.422628Z",
          "iopub.execute_input": "2025-03-25T20:13:08.422956Z",
          "iopub.status.idle": "2025-03-25T20:13:23.803794Z",
          "shell.execute_reply.started": "2025-03-25T20:13:08.42293Z",
          "shell.execute_reply": "2025-03-25T20:13:23.802599Z"
        },
        "id": "iE-4lfiKSuTv"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Pre-Processing**"
      ],
      "metadata": {
        "id": "0KTE8vowSuTv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lowercasing All Text\n",
        "\n",
        "train_df['Text'] = train_df['Text'].str.lower()\n",
        "test_df['Text'] = test_df['Text'].str.lower()\n",
        "val_df['Text'] = val_df['Text'].str.lower()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-25T20:13:23.80699Z",
          "iopub.execute_input": "2025-03-25T20:13:23.807516Z",
          "iopub.status.idle": "2025-03-25T20:13:23.862618Z",
          "shell.execute_reply.started": "2025-03-25T20:13:23.807475Z",
          "shell.execute_reply": "2025-03-25T20:13:23.861612Z"
        },
        "id": "29LXGhalSuTw"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand abbreviations\n",
        "\n",
        "abbreviations = {\n",
        "    \"brb\": \"be right back\",\n",
        "    \"tmr\": \"tomorrow\",\n",
        "    \"omg\": \"oh my god\",\n",
        "    \"lol\": \"laugh out loud\",\n",
        "    \"btw\": \"by the way\",\n",
        "    \"idk\": \"i don't know\",\n",
        "    \"imo\": \"in my opinion\",\n",
        "    \"smh\": \"shaking my head\",\n",
        "    \"tbh\": \"to be honest\",\n",
        "    \"afaik\": \"as far as i know\",\n",
        "    \"irl\": \"in real life\",\n",
        "    \"rofl\": \"rolling on the floor laughing\",\n",
        "    \"thx\": \"thanks\",\n",
        "    \"np\": \"no problem\",\n",
        "    \"yw\": \"you're welcome\",\n",
        "    \"idc\": \"i don't care\",\n",
        "    \"ikr\": \"i know, right\",\n",
        "    \"ofc\": \"of course\",\n",
        "    \"pls\": \"please\",\n",
        "    \"sry\": \"sorry\",\n",
        "    \"wbu\": \"what about you\",\n",
        "    \"w/o\": \"without\",\n",
        "    \"w/\": \"with\",\n",
        "    \"rly\": \"really\",\n",
        "    \"u\": \"you\",\n",
        "    \"ur\": \"your\",\n",
        "    \"2\": \"to\",\n",
        "    \"4\": \"for\",\n",
        "    \"b4\": \"before\",\n",
        "    \"gr8\": \"great\",\n",
        "    \"bday\": \"birthday\",\n",
        "    \"bff\": \"best friends forever\",\n",
        "    \"fyi\": \"for your information\",\n",
        "    \"gtg\": \"got to go\",\n",
        "    \"hbu\": \"how about you\",\n",
        "    \"jk\": \"just kidding\",\n",
        "    \"k\": \"okay\",\n",
        "    \"nvm\": \"never mind\",\n",
        "    \"omw\": \"on my way\",\n",
        "    \"ttyl\": \"talk to you later\",\n",
        "    \"ty\": \"thank you\",\n",
        "    \"wtf\": \"what the fuck\",\n",
        "    \"yolo\": \"you only live once\",\n",
        "    \"asap\": \"as soon as possible\",\n",
        "    \"atm\": \"at the moment\",\n",
        "    \"b/c\": \"because\",\n",
        "    \"fyi\": \"for your information\",\n",
        "    \"icymi\": \"in case you missed it\",\n",
        "    \"tmi\": \"too much information\",\n",
        "    \"ftw\": \"for the win\",\n",
        "    \"ily\": \"i love you\",\n",
        "    \"ily2\": \"i love you too\",\n",
        "    \"imho\": \"in my humble opinion\",\n",
        "    \"nbd\": \"no big deal\",\n",
        "    \"tbt\": \"throwback thursday\",\n",
        "    \"tgif\": \"thank god it's friday\",\n",
        "    \"ykyk\": \"you know you know\",\n",
        "    \"rn\": \"right now\",\n",
        "    \"tba\": \"to be announced\",\n",
        "    \"tbd\": \"to be determined\",\n",
        "    \"nsfw\": \"not safe for work\",\n",
        "    \"sfw\": \"safe for work\",\n",
        "    \"eli5\": \"explain like i'm five\",\n",
        "    \"tl;dr\": \"too long; didn't read\",\n",
        "    \"gg\": \"good game\",\n",
        "    \"glhf\": \"good luck, have fun\",\n",
        "    \"afk\": \"away from keyboard\",\n",
        "    \"op\": \"overpowered\",\n",
        "    \"nerf\": \"weaken a game element\",\n",
        "    \"buff\": \"strengthen a game element\",\n",
        "    \"lfg\": \"looking for group\",\n",
        "    \"dps\": \"damage per second\",\n",
        "    \"aoe\": \"area of effect\",\n",
        "    \"eod\": \"end of day\",\n",
        "    \"eom\": \"end of message\",\n",
        "    \"kpi\": \"key performance indicator\",\n",
        "    \"roi\": \"return on investment\",\n",
        "    \"api\": \"application programming interface\",\n",
        "    \"ssh\": \"secure shell\",\n",
        "    \"http\": \"hypertext transfer protocol\",\n",
        "    \"url\": \"uniform resource locator\",\n",
        "    \"os\": \"operating system\",\n",
        "    \"cpu\": \"central processing unit\",\n",
        "    \"gpu\": \"graphics processing unit\",\n",
        "    \"iot\": \"internet of things\",\n",
        "    \"ai\": \"artificial intelligence\",\n",
        "    \"ml\": \"machine learning\",\n",
        "    \"vpn\": \"virtual private network\",\n",
        "    \"html\": \"hypertext markup language\",\n",
        "    \"css\": \"cascading style sheets\",\n",
        "    \"sql\": \"structured query language\",\n",
        "    \"otc\": \"over the counter\",\n",
        "    \"rx\": \"prescription\",\n",
        "    \"er\": \"emergency room\",\n",
        "    \"icu\": \"intensive care unit\",\n",
        "    \"adhd\": \"attention deficit hyperactivity disorder\",\n",
        "    \"apr\": \"annual percentage rate\",\n",
        "    \"ira\": \"individual retirement account\",\n",
        "    \"nyse\": \"new york stock exchange\",\n",
        "    \"sec\": \"securities and exchange commission\",\n",
        "    \"etf\": \"exchange-traded fund\",\n",
        "    \"gdp\": \"gross domestic product\",\n",
        "    \"phd\": \"doctor of philosophy\",\n",
        "    \"ma\": \"master of arts\",\n",
        "    \"ms\": \"master of science\",\n",
        "    \"gpa\": \"grade point average\",\n",
        "    \"sat\": \"scholastic assessment test\",\n",
        "    \"gre\": \"graduate record examination\",\n",
        "    \"ta\": \"teaching assistant\",\n",
        "    \"fwb\": \"friends with benefits\",\n",
        "    \"nsa\": \"no strings attached\",\n",
        "    \"ship\": \"relationship\",\n",
        "    \"dtr\": \"define the relationship\",\n",
        "    \"ghosting\": \"ending contact abruptly\",\n",
        "    \"bnb\": \"bed and breakfast\",\n",
        "    \"visa\": \"travel permit\",\n",
        "    \"tsa\": \"transportation security administration\",\n",
        "    \"gf\": \"gluten-free\",\n",
        "    \"vegan\": \"no animal products\",\n",
        "    \"veg\": \"vegetarian\",\n",
        "    \"byob\": \"bring your own bottle\",\n",
        "    \"mvp\": \"most valuable player\",\n",
        "    \"qb\": \"quarterback\",\n",
        "    \"nfl\": \"national football league\",\n",
        "    \"nba\": \"national basketball association\",\n",
        "    \"bpm\": \"beats per minute\",\n",
        "    \"dj\": \"disc jockey\",\n",
        "    \"mv\": \"music video\",\n",
        "    \"sahm\": \"stay-at-home mom\",\n",
        "    \"blw\": \"baby-led weaning\",\n",
        "    \"pov\": \"point of view\",\n",
        "    \"fomo\": \"fear of missing out\",\n",
        "    \"hmu\": \"hit me up\",\n",
        "    \"lmk\": \"let me know\",\n",
        "    \"fwiw\": \"for what it's worth\",\n",
        "    \"am\": \"ante meridiem (before noon)\",\n",
        "    \"pm\": \"post meridiem (after noon)\",\n",
        "    \"diy\": \"do it yourself\",\n",
        "    \"eta\": \"estimated time of arrival\",\n",
        "    \"rsvp\": \"répondez s'il vous plaît (please respond)\",\n",
        "    \"asl\": \"age/sex/location\",\n",
        "    \"bbl\": \"be back later\",\n",
        "    \"bbl8r\": \"be back later\",\n",
        "    \"bae\": \"before anyone else\",\n",
        "    \"cap\": \"lying\",\n",
        "    \"cmv\": \"change my view\",\n",
        "    \"ctfu\": \"cracking the fuck up\",\n",
        "    \"dafuq\": \"what the fuck\",\n",
        "    \"doxx\": \"expose personal info\",\n",
        "    \"fml\": \"fuck my life\",\n",
        "    \"ft\": \"featuring\",\n",
        "    \"gl\": \"good luck\",\n",
        "    \"hth\": \"hope this helps\",\n",
        "    \"iirc\": \"if i recall correctly\",\n",
        "    \"jsyk\": \"just so you know\",\n",
        "    \"mcm\": \"man crush monday\",\n",
        "    \"mood\": \"relatable feeling\",\n",
        "    \"mrw\": \"my reaction when\",\n",
        "    \"mtfbwy\": \"may the force be with you\",\n",
        "    \"n/a\": \"not applicable\",\n",
        "    \"ootd\": \"outfit of the day\",\n",
        "    \"pov\": \"point of view\",\n",
        "    \"roflmao\": \"rolling on the floor laughing my ass off\",\n",
        "    \"smd\": \"suck my dick\",\n",
        "    \"sus\": \"suspicious\",\n",
        "    \"tbf\": \"to be fair\",\n",
        "    \"tfw\": \"that feeling when\",\n",
        "    \"woke\": \"socially aware\",\n",
        "    \"yeet\": \"throw forcefully\",\n",
        "    \"zerg\": \"overwhelm with numbers (gaming)\",\n",
        "    \"404\": \"not found\",\n",
        "    \"1337\": \"elite (leet)\",\n",
        "    \"143\": \"i love you\",\n",
        "    \"459\": \"i love you (keypad letters)\",\n",
        "    \"irl\": \"in real life\",\n",
        "    \"f2f\": \"face to face\",\n",
        "    \"irl\": \"in real life\",\n",
        "    \"ama\": \"ask me anything\",\n",
        "    \"dm\": \"direct message\",\n",
        "    \"dnd\": \"do not disturb\",\n",
        "    \"fbf\": \"flashback friday\",\n",
        "    \"wcw\": \"woman crush wednesday\",\n",
        "    \"tgm\": \"thank goodness it's monday\",\n",
        "    \"pita\": \"pain in the ass\",\n",
        "    \"pos\": \"parent over shoulder\",\n",
        "    \"qap\": \"quick as possible\",\n",
        "    \"qotd\": \"quote of the day\",\n",
        "    \"roflcopter\": \"laughing excessively\",\n",
        "    \"smdh\": \"shaking my damn head\",\n",
        "    \"srs\": \"serious\",\n",
        "    \"stfu\": \"shut the fuck up\",\n",
        "    \"swak\": \"sealed with a kiss\",\n",
        "    \"tf\": \"the fuck\",\n",
        "    \"thot\": \"that hoe over there\",\n",
        "    \"tia\": \"thanks in advance\",\n",
        "    \"til\": \"today i learned\",\n",
        "    \"tl\": \"timeline\",\n",
        "    \"tmi\": \"too much information\",\n",
        "    \"ttfn\": \"ta-ta for now\",\n",
        "    \"ttul\": \"talk to you later\",\n",
        "    \"tyt\": \"take your time\",\n",
        "    \"wyd\": \"what you doing\",\n",
        "    \"wys\": \"what you saying\",\n",
        "    \"ymmv\": \"your mileage may vary\",\n",
        "    \"yolo\": \"you only live once\",\n",
        "    \"zzz\": \"sleeping or bored\",\n",
        "}\n",
        "\n",
        "def replace_abbreviations(text):\n",
        "    words = text.split()\n",
        "    expanded_words = [abbreviations.get(word, word) for word in words]\n",
        "    return ' '.join(expanded_words)\n",
        "\n",
        "train_df['Text'] = train_df['Text'].apply(replace_abbreviations)\n",
        "val_df['Text'] = val_df['Text'].apply(replace_abbreviations)\n",
        "test_df['Text'] = test_df['Text'].apply(replace_abbreviations)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-25T20:13:23.864427Z",
          "iopub.execute_input": "2025-03-25T20:13:23.86478Z",
          "iopub.status.idle": "2025-03-25T20:13:24.585992Z",
          "shell.execute_reply.started": "2025-03-25T20:13:23.86475Z",
          "shell.execute_reply": "2025-03-25T20:13:24.584874Z"
        },
        "id": "JMRpkeZESuTw"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contractions\n",
        "\n",
        "def expand_contractions(text):\n",
        "    return fix(text)\n",
        "\n",
        "train_df[\"Text\"] = train_df[\"Text\"].apply(expand_contractions)\n",
        "val_df[\"Text\"] = val_df[\"Text\"].apply(expand_contractions)\n",
        "test_df[\"Text\"] = test_df[\"Text\"].apply(expand_contractions)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-25T20:13:24.587022Z",
          "iopub.execute_input": "2025-03-25T20:13:24.58732Z",
          "iopub.status.idle": "2025-03-25T20:13:26.339768Z",
          "shell.execute_reply.started": "2025-03-25T20:13:24.587297Z",
          "shell.execute_reply": "2025-03-25T20:13:26.338881Z"
        },
        "id": "grGYPrsqSuTw"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs, Mentions, Mantaining Only Alphanumeric And Undercores\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r\"http\\S+|@\\w+\", \"\", text)\n",
        "    text = re.sub(r\"[^a-zA-Z0-9_]\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "train_df[\"Text\"] = train_df[\"Text\"].apply(clean_text)\n",
        "val_df[\"Text\"] = val_df[\"Text\"].apply(clean_text)\n",
        "test_df[\"Text\"] = test_df[\"Text\"].apply(clean_text)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-25T20:13:26.340744Z",
          "iopub.execute_input": "2025-03-25T20:13:26.341016Z",
          "iopub.status.idle": "2025-03-25T20:13:29.232487Z",
          "shell.execute_reply.started": "2025-03-25T20:13:26.340994Z",
          "shell.execute_reply": "2025-03-25T20:13:29.231352Z"
        },
        "id": "vh_b7zacSuTx"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove All Emojis\n",
        "\n",
        "def demojize(text):\n",
        "    return emoji.demojize(text, delimiters=(\" \", \" \"))\n",
        "\n",
        "train_df[\"Text\"] = train_df[\"Text\"].apply(demojize)\n",
        "val_df[\"Text\"] = val_df[\"Text\"].apply(demojize)\n",
        "test_df[\"Text\"] = test_df[\"Text\"].apply(demojize)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-25T20:13:29.233558Z",
          "iopub.execute_input": "2025-03-25T20:13:29.23387Z",
          "iopub.status.idle": "2025-03-25T20:13:42.450237Z",
          "shell.execute_reply.started": "2025-03-25T20:13:29.233841Z",
          "shell.execute_reply": "2025-03-25T20:13:42.449064Z"
        },
        "id": "g42zuFXpSuTx"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations\n",
        "\n",
        "def remove_punctuations(text):\n",
        "    translator = str.maketrans(\"\", \"\", string.punctuation)\n",
        "    return text.translate(translator)\n",
        "\n",
        "train_df[\"Text\"] = train_df[\"Text\"].apply(remove_punctuations)\n",
        "val_df[\"Text\"] = val_df[\"Text\"].apply(remove_punctuations)\n",
        "test_df[\"Text\"] = test_df[\"Text\"].apply(remove_punctuations)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-25T20:13:42.451359Z",
          "iopub.execute_input": "2025-03-25T20:13:42.451761Z",
          "iopub.status.idle": "2025-03-25T20:13:43.325208Z",
          "shell.execute_reply.started": "2025-03-25T20:13:42.451719Z",
          "shell.execute_reply": "2025-03-25T20:13:43.324234Z"
        },
        "id": "57TMhfJFSuTx"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply Stemming using Porter\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def stemming(text):\n",
        "    tokens = word_tokenize(str(text))\n",
        "    return \" \".join([stemmer.stem(token) for token in tokens])\n",
        "\n",
        "train_df['Text'] = train_df['Text'].apply(stemming)\n",
        "val_df['Text'] = val_df['Text'].apply(stemming)\n",
        "test_df['Text'] = test_df['Text'].apply(stemming)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-25T20:13:43.326094Z",
          "iopub.execute_input": "2025-03-25T20:13:43.326473Z",
          "iopub.status.idle": "2025-03-25T20:14:53.078226Z",
          "shell.execute_reply.started": "2025-03-25T20:13:43.326441Z",
          "shell.execute_reply": "2025-03-25T20:14:53.077215Z"
        },
        "id": "iXiLHNG3SuTx"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DATA ANALYSIS**"
      ],
      "metadata": {
        "id": "UqDeVUNaSuTx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get The Most Common Words And The Number Of Unique And Total Words After Preprocessing\n",
        "print(\"ANALYSING DATA AFTER PRE-PROCESSING THE DATASET\\n\")\n",
        "\n",
        "def get_most_common_words(text_series, top_n=10, exclude_stopwords=True):\n",
        "    all_words = []\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    for text in text_series.dropna():\n",
        "        tokens = word_tokenize(text)  # Removed lowercasing\n",
        "\n",
        "        if exclude_stopwords:\n",
        "            # Case-insensitive stopword check while preserving original case\n",
        "            words = [word for word in tokens if word.lower() not in stop_words]\n",
        "        else:\n",
        "            words = tokens  # Keep all tokens including punctuation\n",
        "\n",
        "        all_words.extend(words)\n",
        "\n",
        "    word_counts = Counter(all_words)\n",
        "    return word_counts.most_common(top_n)\n",
        "\n",
        "def count_words(text_series, exclude_stopwords=True, unique=True):\n",
        "    all_words = []\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    for text in text_series.dropna():\n",
        "        tokens = word_tokenize(text)  # Removed lowercasing\n",
        "\n",
        "        if exclude_stopwords:\n",
        "            words = [word for word in tokens if word.lower() not in stop_words]\n",
        "        else:\n",
        "            words = tokens\n",
        "\n",
        "        all_words.extend(words)\n",
        "\n",
        "    return len(set(all_words)) if unique else len(all_words)\n",
        "\n",
        "\n",
        "common_words = get_most_common_words(train_df['Text'], top_n=10, exclude_stopwords=False)\n",
        "df = pd.DataFrame(common_words, columns=[\"Word\", \"Frequency\"])\n",
        "\n",
        "print(\"For the training dataset:\\n\", df.to_string(index=False), \"\\n\")\n",
        "\n",
        "common_words = get_most_common_words(val_df['Text'], top_n=10, exclude_stopwords=False)\n",
        "df = pd.DataFrame(common_words, columns=[\"Word\", \"Frequency\"])\n",
        "print(\"For the validation dataset:\\n\", df.to_string(index=False), \"\\n\")\n",
        "\n",
        "common_words = get_most_common_words(test_df['Text'], top_n=10, exclude_stopwords=False)\n",
        "df = pd.DataFrame(common_words, columns=[\"Word\", \"Frequency\"])\n",
        "print(\"For the testing dataset:\\n\", df.to_string(index=False), \"\\n\")\n",
        "\n",
        "vocab_size = count_words(train_df['Text'], exclude_stopwords=False, unique=True)\n",
        "print(f\"Unique word count in the training dataset: {vocab_size}\")\n",
        "\n",
        "vocab_size = count_words(train_df['Text'], exclude_stopwords=False, unique=False)\n",
        "print(f\"Total word count in the training dataset: {vocab_size}\\n\")\n",
        "\n",
        "vocab_size = count_words(val_df['Text'],exclude_stopwords=False, unique=True)\n",
        "print(f\"Unique word count in the validation dataset: {vocab_size}\")\n",
        "\n",
        "vocab_size = count_words(val_df['Text'],exclude_stopwords=False, unique=False)\n",
        "print(f\"Total word count in the validation dataset: {vocab_size}\\n\")\n",
        "\n",
        "vocab_size = count_words(test_df['Text'], exclude_stopwords=False, unique=True)\n",
        "print(f\"Unique word count in the testing dataset: {vocab_size}\")\n",
        "\n",
        "vocab_size = count_words(test_df['Text'],exclude_stopwords=False, unique=False)\n",
        "print(f\"Total word count in the testing dataset: {vocab_size}\\n\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-25T20:14:53.079239Z",
          "iopub.execute_input": "2025-03-25T20:14:53.079635Z",
          "iopub.status.idle": "2025-03-25T20:16:01.226327Z",
          "shell.execute_reply.started": "2025-03-25T20:14:53.079578Z",
          "shell.execute_reply": "2025-03-25T20:16:01.225182Z"
        },
        "id": "619lT_udSuTx"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate Word Cloud After Pre-Processing\n",
        "\n",
        "text = \" \".join(str(word) for word in train_df['Text'])\n",
        "\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.savefig('/kaggle/working/train_word_cloud_after.png')\n",
        "plt.show()\n",
        "\n",
        "text = \" \".join(str(word) for word in val_df['Text'])\n",
        "\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.savefig('/kaggle/working/val_word_cloud_after.png')\n",
        "plt.show()\n",
        "\n",
        "text = \" \".join(str(word) for word in test_df['Text'])\n",
        "\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.savefig('/kaggle/working/test_word_cloud_after.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-25T20:16:01.227392Z",
          "iopub.execute_input": "2025-03-25T20:16:01.227791Z",
          "iopub.status.idle": "2025-03-25T20:16:14.357335Z",
          "shell.execute_reply.started": "2025-03-25T20:16:01.227761Z",
          "shell.execute_reply": "2025-03-25T20:16:14.356264Z"
        },
        "id": "zzfLAFsaSuTy"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TF-IDF Vectorizer**"
      ],
      "metadata": {
        "id": "6cjmSHlSSuTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement the TF-IDF Vectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(\n",
        "    max_features=100000,\n",
        "    ngram_range=(1,2),\n",
        "    min_df=2,\n",
        "    max_df=0.95,\n",
        "    sublinear_tf=True,\n",
        "    use_idf=True,\n",
        "    smooth_idf=True,\n",
        ")\n",
        "\n",
        "X_train_tfidf = vectorizer.fit_transform(train_df[\"Text\"])\n",
        "\n",
        "X_val_tfidf = vectorizer.transform(val_df[\"Text\"])\n",
        "X_test_tfidf = vectorizer.transform(test_df[\"Text\"])\n",
        "\n",
        "y_train = train_df[\"Label\"]\n",
        "y_val = val_df[\"Label\"]"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-25T20:16:14.358423Z",
          "iopub.execute_input": "2025-03-25T20:16:14.358844Z",
          "iopub.status.idle": "2025-03-25T20:16:22.592145Z",
          "shell.execute_reply.started": "2025-03-25T20:16:14.358811Z",
          "shell.execute_reply": "2025-03-25T20:16:22.591102Z"
        },
        "id": "yYPX3o4BSuTy"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Logistic Regression**"
      ],
      "metadata": {
        "id": "bRAQr0KvSuTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement the Logistic Regression model\n",
        "\n",
        "model = LogisticRegression( max_iter=1000, C=0.95,random_state=RANDOMSEED)\n",
        "\n",
        "model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "y_pred = model.predict(X_val_tfidf)\n",
        "\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "print(f\"Validation Accuracy:{accuracy:.5f}\")\n",
        "print(\"Classification Report:\\n\", classification_report(y_val, y_pred, digits=5))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-25T20:16:22.593021Z",
          "iopub.execute_input": "2025-03-25T20:16:22.593297Z",
          "iopub.status.idle": "2025-03-25T20:16:27.619094Z",
          "shell.execute_reply.started": "2025-03-25T20:16:22.593274Z",
          "shell.execute_reply": "2025-03-25T20:16:27.617926Z"
        },
        "id": "jqmcSVr6SuTy"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Submit The Model**"
      ],
      "metadata": {
        "id": "RZIh1AmZSuTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Submit the prediction to Kaggle\n",
        "\n",
        "test_predictions = model.predict(X_test_tfidf)\n",
        "\n",
        "submission = pd.DataFrame({\"ID\": test_df[\"ID\"], \"Label\": test_predictions})\n",
        "\n",
        "submission.to_csv(\"submission.csv\", index=False)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-25T20:16:27.620332Z",
          "iopub.execute_input": "2025-03-25T20:16:27.620634Z",
          "iopub.status.idle": "2025-03-25T20:16:27.652799Z",
          "shell.execute_reply.started": "2025-03-25T20:16:27.620609Z",
          "shell.execute_reply": "2025-03-25T20:16:27.651781Z"
        },
        "id": "pbEZ_wGzSuTz"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Analyze The Model**"
      ],
      "metadata": {
        "id": "U9sAqJDQSuTz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the Absolute Difference (Percentage) Between Training Score and Accuracy Score\n",
        "\n",
        "y_train_pred = model.predict(X_train_tfidf)\n",
        "\n",
        "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "val_accuracy = accuracy_score(y_val, y_pred)\n",
        "\n",
        "absolute_difference = abs((train_accuracy - val_accuracy) * 100)\n",
        "\n",
        "print(f\"Training Accuracy: {train_accuracy:.5f}\")\n",
        "print(f\"Validation Accuracy: {val_accuracy:.5f}\")\n",
        "print(f\"\\nAbsolute Difference (Percentage): {absolute_difference:.5f}%\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-25T20:16:27.653816Z",
          "iopub.execute_input": "2025-03-25T20:16:27.654209Z",
          "iopub.status.idle": "2025-03-25T20:16:27.694237Z",
          "shell.execute_reply.started": "2025-03-25T20:16:27.65417Z",
          "shell.execute_reply": "2025-03-25T20:16:27.693299Z"
        },
        "id": "_3tDZfuCSuTz"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the Absolute Difference (Percentage) Between Training Log Loss and Accuracy Log Loss\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "y_train_proba = model.predict_proba(X_train_tfidf)\n",
        "y_val_proba = model.predict_proba(X_val_tfidf)\n",
        "\n",
        "train_log_loss = log_loss(y_train, y_train_proba)\n",
        "val_log_loss = log_loss(y_val, y_val_proba)\n",
        "\n",
        "absolute_difference = abs((train_log_loss - val_log_loss) * 100)\n",
        "\n",
        "print(f\"Train Log Loss: {train_log_loss:.4f}\")\n",
        "print(f\"Validation Log Loss: {val_log_loss:.4f}\")\n",
        "print(f\"\\nAbsolute Difference (Percentage): {absolute_difference:.5f}%\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-25T20:16:27.695421Z",
          "iopub.execute_input": "2025-03-25T20:16:27.695751Z",
          "iopub.status.idle": "2025-03-25T20:16:27.772543Z",
          "shell.execute_reply.started": "2025-03-25T20:16:27.695715Z",
          "shell.execute_reply": "2025-03-25T20:16:27.771509Z"
        },
        "id": "kNAv0K8fSuTz"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate Receiver operating characteristic (ROC) Curve\n",
        "\n",
        "y_pred_proba = model.predict_proba(X_val_tfidf)[:, 1]\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(y_val, y_pred_proba)\n",
        "roc_auc = roc_auc_score(y_val, y_pred_proba)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Guessing')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate (FPR)')\n",
        "plt.ylabel('True Positive Rate (TPR)')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.savefig('/kaggle/working/roc_curve.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-25T20:16:27.776143Z",
          "iopub.execute_input": "2025-03-25T20:16:27.77645Z",
          "iopub.status.idle": "2025-03-25T20:16:28.165209Z",
          "shell.execute_reply.started": "2025-03-25T20:16:27.776426Z",
          "shell.execute_reply": "2025-03-25T20:16:28.164203Z"
        },
        "id": "GnoNzefpSuTz"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate Confusion Matrix\n",
        "\n",
        "cm = confusion_matrix(y_val, y_pred)\n",
        "\n",
        "labels = [\"0\", \"1\"]\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=labels, yticklabels=labels)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.savefig('/kaggle/working/confusion_matrix.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-25T20:16:28.166426Z",
          "iopub.execute_input": "2025-03-25T20:16:28.166728Z",
          "iopub.status.idle": "2025-03-25T20:16:28.457253Z",
          "shell.execute_reply.started": "2025-03-25T20:16:28.166682Z",
          "shell.execute_reply": "2025-03-25T20:16:28.45624Z"
        },
        "id": "_i7dyGXlSuTz"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate learning curve\n",
        "\n",
        "X_combined = vstack([X_train_tfidf, X_val_tfidf])\n",
        "y_combined = np.concatenate([y_train, y_val])\n",
        "\n",
        "n_train = X_train_tfidf.shape[0]\n",
        "n_val = X_val_tfidf.shape[0]\n",
        "\n",
        "test_fold = np.array([-1] * n_train + [0] * n_val)\n",
        "\n",
        "custom_cv = PredefinedSplit(test_fold)\n",
        "\n",
        "test_fold = np.concatenate((-np.ones(len(y_train)), np.zeros(len(y_val))))\n",
        "\n",
        "ps = PredefinedSplit(test_fold)\n",
        "\n",
        "train_sizes, train_scores, test_scores = learning_curve(\n",
        "    model,\n",
        "    X_combined,\n",
        "    y_combined,\n",
        "    cv=custom_cv,\n",
        "    scoring=\"accuracy\",\n",
        "    train_sizes=np.linspace(0.1, 1.0, 10)\n",
        ")\n",
        "\n",
        "train_mean = np.mean(train_scores, axis=1)\n",
        "train_std = np.std(train_scores, axis=1)\n",
        "test_mean = np.mean(test_scores, axis=1)\n",
        "test_std = np.std(test_scores, axis=1)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(train_sizes, train_mean, label=\"Training Score\", color=\"blue\", marker=\"o\")\n",
        "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color=\"blue\")\n",
        "\n",
        "plt.plot(train_sizes, test_mean, label=\"Validation Score\", color=\"red\", marker=\"s\")\n",
        "plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.1, color=\"red\")\n",
        "\n",
        "plt.xlabel(\"Training Size\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Learning Curve for Logistic Regression\")\n",
        "plt.legend()\n",
        "plt.savefig('/kaggle/working/learning_curve.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-25T20:16:28.458295Z",
          "iopub.execute_input": "2025-03-25T20:16:28.458631Z",
          "iopub.status.idle": "2025-03-25T20:16:56.060838Z",
          "shell.execute_reply.started": "2025-03-25T20:16:28.458597Z",
          "shell.execute_reply": "2025-03-25T20:16:56.059779Z"
        },
        "id": "MUbpy8uUSuTz"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}